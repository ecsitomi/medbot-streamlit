# =============================================================================
# rag_pdf/rag_analyzer.py - JAV√çTOTT VERZI√ì
# =============================================================================
"""
RAG alap√∫ PDF elemz√©s JAV√çTOTT VERZI√ì - deprecated f√ºggv√©nyek kijav√≠tva
"""
import os
import streamlit as st
from typing import List, Dict, Any, Optional
from datetime import datetime
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
#from langchain_chroma import Chroma
import json
from pathlib import Path

# Streamlitre kell
try:
    import pysqlite3
    import sys
    sys.modules["sqlite3"] = pysqlite3
except ImportError:
    print("‚ö†Ô∏è pysqlite3-binary nem el√©rhet≈ë ‚Äì SQLite override sikertelen")

# Ford√≠t√°si seg√©df√ºggv√©nyek
def translate_text(text: str, openai_api_key: str) -> str:
    if not text:
        return ""
    llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0)
    return llm.predict(f"Translate this Hungarian medical phrase to English: {text}").strip()

def translate_list(items: List[str], openai_api_key: str) -> List[str]:
    return [translate_text(item, openai_api_key) for item in items if item]

def translate_patient_data(patient_data: Dict[str, Any], openai_api_key: str) -> Dict[str, Any]:
    translated = patient_data.copy()
    translated['symptoms'] = translate_list(patient_data.get('symptoms', []), openai_api_key)
    translated['diagnosis'] = translate_text(patient_data.get('diagnosis', ''), openai_api_key)
    translated['existing_conditions'] = translate_list(patient_data.get('existing_conditions', []), openai_api_key)
    translated['medications'] = translate_list(patient_data.get('medications', []), openai_api_key)
    return translated

###

class RAGAnalyzer:
    """
    RAG alap√∫ PDF elemz√©s JAV√çTOTT VERZI√ì
    """
    
    def __init__(self, vector_store_path: str = "rag_pdf/vectorstore"):
        self.vector_store_path = vector_store_path
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.retrieval_chain = None
        self._initialize_components()
    
    def _initialize_components(self):
        """Komponensek inicializ√°l√°sa JAV√çTOTT verzi√≥"""
        try:
            # ‚úÖ JAV√çTVA: OpenAI API key kezel√©s
            api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OpenAI API key not found")
            
            # ‚úÖ JAV√çTVA: Modern LangChain komponensek
            self.embeddings = OpenAIEmbeddings(
                openai_api_key=api_key,
                model="text-embedding-3-small"  # Leg√∫jabb embedding model
            )
            
            self.llm = ChatOpenAI(
                openai_api_key=api_key,
                model="gpt-4",
                temperature=0.1,
                max_tokens=1500
            )
            
            # Vector store bet√∂lt√©se vagy l√©trehoz√°sa
            self._load_or_create_vectorstore()
            
            # ‚úÖ JAV√çTVA: Modern LCEL (LangChain Expression Language) chain
            self._create_retrieval_chain()
            
            print("‚úÖ RAG Analyzer sikeresen inicializ√°lva")
            
        except Exception as e:
            print(f"‚ùå RAG Analyzer inicializ√°l√°si hiba: {e}")
            raise
    
    def _load_or_create_vectorstore(self):
        """Vector store bet√∂lt√©se vagy l√©trehoz√°sa"""
        try:
            if os.path.exists(self.vector_store_path):
                # ‚úÖ JAV√çTVA: Bet√∂lt√©s ellen≈ërz√©se
                self.vectorstore = Chroma(
                    persist_directory=self.vector_store_path,
                    embedding_function=self.embeddings
                )
                
                # Ellen≈ërizz√ºk, hogy van-e tartalom
                collection_count = self.vectorstore._collection.count()
                if collection_count > 0:
                    print(f"‚úÖ Vector store bet√∂ltve: {collection_count} dokumentum")
                else:
                    print("‚ö†Ô∏è Vector store √ºres, PDF-ek bet√∂lt√©se sz√ºks√©ges")
                    self._load_pdfs_to_vectorstore()
            else:
                print("üìÅ Vector store nem l√©tezik, l√©trehoz√°s...")
                self._load_pdfs_to_vectorstore()
                
        except Exception as e:
            print(f"‚ùå Vector store hiba: {e}")
            # Fallback: √∫j vector store l√©trehoz√°sa
            self._load_pdfs_to_vectorstore()
    
    def _load_pdfs_to_vectorstore(self):
        """PDF-ek bet√∂lt√©se a vector store-ba JAV√çTOTT verzi√≥"""
        try:
            # ‚úÖ JAV√çTVA: PDF k√∂nyvt√°r ellen≈ërz√©se
            pdf_directory = Path("medline_data/pdfs")
            if not pdf_directory.exists():
                print(f"‚ùå PDF k√∂nyvt√°r nem l√©tezik: {pdf_directory}")
                return
            
            pdf_files = list(pdf_directory.glob("*.pdf"))
            if not pdf_files:
                print("‚ùå Nincsenek PDF f√°jlok a k√∂nyvt√°rban")
                return
            
            print(f"üìö PDF f√°jlok bet√∂lt√©se: {len(pdf_files)} f√°jl")
            
            # ‚úÖ JAV√çTVA: Dokumentumok feldolgoz√°sa
            all_documents = []
            
            for pdf_file in pdf_files:
                try:
                    # PDF bet√∂lt√©se
                    loader = PyPDFLoader(str(pdf_file))
                    documents = loader.load()
                    
                    # Metadata hozz√°ad√°sa
                    for doc in documents:
                        doc.metadata.update({
                            'source_file': pdf_file.name,
                            'file_type': 'medline_pdf',
                            'topic': self._extract_topic_from_filename(pdf_file.name)
                        })
                    
                    all_documents.extend(documents)
                    print(f"‚úÖ Bet√∂ltve: {pdf_file.name} ({len(documents)} oldal)")
                    
                except Exception as e:
                    print(f"‚ùå Hiba PDF bet√∂lt√©sekor ({pdf_file.name}): {e}")
            
            if not all_documents:
                print("‚ùå Nincsenek bet√∂lthet≈ë dokumentumok")
                return
            
            # ‚úÖ JAV√çTVA: Text splitting optimaliz√°l√°sa
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,      # Kisebb chunk-ok a pontosabb retrieval√©rt
                chunk_overlap=200,    # √Åtfed√©s a kontextus meg≈ërz√©s√©re
                length_function=len,
                separators=["\n\n", "\n", ". ", " ", ""]
            )
            
            split_documents = text_splitter.split_documents(all_documents)
            print(f"üìÑ Dokumentumok feldarabolva: {len(split_documents)} chunk")
            
            # ‚úÖ JAV√çTVA: Vector store l√©trehoz√°sa
            os.makedirs(self.vector_store_path, exist_ok=True)
            
            self.vectorstore = Chroma.from_documents(
                documents=split_documents,
                embedding=self.embeddings,
                persist_directory=self.vector_store_path
            )
            
            print(f"‚úÖ Vector store l√©trehozva: {len(split_documents)} dokumentum chunk")
            
        except Exception as e:
            print(f"‚ùå PDF bet√∂lt√©si hiba: {e}")
            raise
    
    def _extract_topic_from_filename(self, filename: str) -> str:
        """Topic kinyer√©se a f√°jln√©vb≈ël"""
        # medline_01_headache_20250730_095014.pdf
        parts = filename.replace('.pdf', '').split('_')
        if len(parts) >= 3:
            return parts[2]  # headache r√©sz
        return "unknown"
    
    def _create_retrieval_chain(self):
        """‚úÖ JAV√çTVA: Modern LCEL chain l√©trehoz√°sa"""
        if not self.vectorstore:
            raise ValueError("Vector store nincs inicializ√°lva")
        
        # ‚úÖ JAV√çTVA: Magyar nyelv≈± prompt template
        prompt_template = PromptTemplate.from_template("""
Te egy eg√©szs√©g√ºgyi szak√©rt≈ë vagy, aki Medline Plus inform√°ci√≥k alapj√°n ad tan√°csokat.

KONTEXTUS (Medline Plus dokumentumok):
{context}

BETEG ADATOK √âS K√âRD√âS: {question}

FELADAT:
V√°laszolj MAGYARUL a k√∂vetkez≈ë k√©rd√©sekre a Medline dokumentumok alapj√°n:

1. **Mi lehet a beteg probl√©m√°ja?** - Diagn√≥zis √©s magyar√°zat
2. **Mit tehet a t√ºnetek ellen?** - Kezel√©si lehet≈ës√©gek √©s otthoni praktik√°k  
3. **Milyen orvoshoz forduljon?** - Specializ√°ci√≥ √©s s√ºrg≈ëss√©g
4. **Tov√°bbi tan√°csok** - Megel≈ëz√©s √©s hasznos inform√°ci√≥k

Ha nincs relev√°ns inform√°ci√≥, √≠rd: "Nincs megfelel≈ë inform√°ci√≥ a dokumentumokban"

V√ÅLASZ MAGYARUL:
""")

        # ‚úÖ JAV√çTVA: Retriever konfigur√°l√°sa
        retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={
                "k": 5,  # Top 5 legrelv√°nsabb chunk
                #"score_threshold": 0.3  # Minimum hasonl√≥s√°gi k√ºsz√∂b
            }
        )
        
        # ‚úÖ JAV√çTVA: Modern LCEL chain (LangChain Expression Language)
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        self.retrieval_chain = (
            {
                "context": retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt_template
            | self.llm
            | StrOutputParser()
        )
        
        print("‚úÖ Modern LCEL retrieval chain l√©trehozva")
    
    def analyze_medical_case(self, case_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Orvosi eset elemz√©se JAV√çTOTT verzi√≥
        
        Args:
            case_data: Orvosi eset adatok (JSON)
            
        Returns:
            Dict: Elemz√©si eredm√©nyek
        """
        try:
            if not self.retrieval_chain:
                raise ValueError("RAG chain nincs inicializ√°lva")
            
            # ‚úÖ JAV√çTVA: Query √∂ssze√°ll√≠t√°sa
            translated_data = translate_patient_data(case_data, os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY"))
            query = self._build_medical_query(translated_data)
            #query = self._build_medical_query(case_data)
            print(f"üîç RAG Query: {query}")
            
            # ‚úÖ JAV√çTVA: Modern invoke haszn√°lata predict helyett
            with st.spinner("RAG elemz√©s folyamatban..."):
                rag_response = self.retrieval_chain.invoke(query)
            
            print(f"üìÑ RAG Response: {rag_response[:200]}...")
            
            # ‚úÖ JAV√çTVA: V√°lasz feldolgoz√°sa
            analysis_result = self._parse_rag_response(rag_response, case_data)
            
            return analysis_result
            
        except Exception as e:
            print(f"‚ùå RAG elemz√©si hiba: {e}")
            return {
                'success': False,
                'error': str(e),
                'rag_response': None,
                'medical_insights': []
            }
    
    def _build_medical_query(self, case_data: Dict[str, Any]) -> str:
        """‚úÖ JAV√çTVA: Orvosi query √∂ssze√°ll√≠t√°sa"""
        # Alapadatok kinyer√©se
        symptoms = case_data.get('symptoms', [])
        age = case_data.get('age', 'ismeretlen')
        gender = case_data.get('gender', 'ismeretlen')
        duration = case_data.get('duration', 'ismeretlen')
        diagnosis = case_data.get('diagnosis', '')
        existing_conditions = case_data.get('existing_conditions', [])
        medications = case_data.get('medications', [])
        
        # Query √∂ssze√°ll√≠t√°sa magyar nyelven
        query_parts = []
        
        query_parts.append(f"P√°ciens: {age} √©ves {gender}")
        
        if symptoms:
            symptoms_text = ', '.join(symptoms)
            query_parts.append(f"T√ºnetek: {symptoms_text}")
        
        if duration and duration != 'ismeretlen':
            query_parts.append(f"Id≈ëtartam: {duration}")
        
        if diagnosis and diagnosis != "Nem siker√ºlt diagn√≥zist javasolni.":
            query_parts.append(f"Lehets√©ges diagn√≥zis: {diagnosis}")
            
        if existing_conditions:
            query_parts.append(f"Megl√©v≈ë betegs√©gek: {', '.join(existing_conditions)}")
            
        if medications:
            query_parts.append(f"Gy√≥gyszerek: {', '.join(medications)}")
        
        main_query = " | ".join(query_parts)
        
        full_query = f"""
{main_query}

K√©rlek adj r√©szletes eg√©szs√©g√ºgyi tan√°csokat a Medline dokumentumok alapj√°n!
"""
        
        return full_query.strip()
    
    def _parse_rag_response(self, rag_response: str, case_data: Dict[str, Any]) -> Dict[str, Any]:
        """‚úÖ JAV√çTVA: RAG v√°lasz feldolgoz√°sa struktur√°lt form√°ba"""
        
        # ‚úÖ JAV√çTVA: √úres v√°lasz ellen≈ërz√©s
        if not rag_response or rag_response.strip() == "":
            return {
                'success': False,
                'error': '√úres RAG v√°lasz',
                'patient_condition': 'Nincs inform√°ci√≥',
                'symptom_management': 'Nincs inform√°ci√≥', 
                'recommended_specialist': 'Nincs inform√°ci√≥',
                'additional_info': 'Nincs inform√°ci√≥',
                'timestamp': datetime.now().isoformat(),
                'sources': []
            }
        
        # ‚úÖ JAV√çTVA: "Nincs relev√°ns inform√°ci√≥" ellen≈ërz√©s
        if "nincs megfelel≈ë inform√°ci√≥" in rag_response.lower() or "nincs relev√°ns inform√°ci√≥" in rag_response.lower():
            return {
                'success': False,
                'error': 'Nincs relev√°ns inform√°ci√≥ a dokumentumokban',
                'patient_condition': 'Nincs megfelel≈ë inform√°ci√≥ a dokumentumokban',
                'symptom_management': 'Nincs megfelel≈ë inform√°ci√≥ a dokumentumokban',
                'recommended_specialist': 'Nincs megfelel≈ë inform√°ci√≥ a dokumentumokban', 
                'additional_info': 'Nincs megfelel≈ë inform√°ci√≥ a dokumentumokban',
                'timestamp': datetime.now().isoformat(),
                'sources': []
            }
        
        # ‚úÖ JAV√çTVA: Struktur√°lt v√°lasz kinyer√©se
        parsed_result = self._extract_structured_response(rag_response)
        
        # ‚úÖ JAV√çTVA: Eredm√©ny strukt√∫ra
        result = {
            'success': True,
            'patient_condition': parsed_result.get('patient_condition', 'Nem siker√ºlt kinyerni'),
            'symptom_management': parsed_result.get('symptom_management', 'Nem siker√ºlt kinyerni'),
            'recommended_specialist': parsed_result.get('recommended_specialist', 'Nem siker√ºlt kinyerni'),
            'additional_info': parsed_result.get('additional_info', 'Nem siker√ºlt kinyerni'),
            'timestamp': datetime.now().isoformat(),
            'sources': ['medline_pdf'],  # √Åltal√°nos forr√°s
            'full_response': rag_response  # Teljes v√°lasz meg≈ërz√©se
        }
        
        return result
    
    def _extract_structured_response(self, response: str) -> Dict[str, str]:
        #Struktur√°lt v√°lasz kinyer√©se a RAG response-b√≥l sorsz√°m alapj√°n

        sections = {
            "patient_condition": "",
            "symptom_management": "",
            "recommended_specialist": "",
            "additional_info": ""
        }

        import re

        # Regex a 4 szekci√≥ c√≠m√©nek megtal√°l√°s√°ra
        pattern = r"(1\.\s\*\*.*?\*\*.*?)(?=2\.|\Z)|" \
                r"(2\.\s\*\*.*?\*\*.*?)(?=3\.|\Z)|" \
                r"(3\.\s\*\*.*?\*\*.*?)(?=4\.|\Z)|" \
                r"(4\.\s\*\*.*?\*\*.*)"

        matches = re.findall(pattern, response, flags=re.DOTALL)

        for i, match_group in enumerate(matches):
            for match in match_group:
                if match:
                    if i == 0:
                        sections["patient_condition"] = match.strip()
                    elif i == 1:
                        sections["symptom_management"] = match.strip()
                    elif i == 2:
                        sections["recommended_specialist"] = match.strip()
                    elif i == 3:
                        sections["additional_info"] = match.strip()

        # Fallback √ºzenet, ha valami kimarad
        for key in sections:
            if not sections[key]:
                sections[key] = "Nem siker√ºlt relev√°ns inform√°ci√≥t tal√°lni"

        return sections

    
    def get_vectorstore_stats(self) -> Dict[str, Any]:
        """Vector store statisztik√°k"""
        try:
            if not self.vectorstore:
                return {'error': 'Vector store nincs inicializ√°lva'}
            
            collection_count = self.vectorstore._collection.count()
            
            # Metadatok √∂sszegy≈±jt√©se
            if collection_count > 0:
                # P√©lda dokumentumok lek√©r√©se
                sample_docs = self.vectorstore.similarity_search("medline", k=3)
                topics = set()
                sources = set()
                
                for doc in sample_docs:
                    if 'topic' in doc.metadata:
                        topics.add(doc.metadata['topic'])
                    if 'source_file' in doc.metadata:
                        sources.add(doc.metadata['source_file'])
                
                return {
                    'total_documents': collection_count,
                    'topics_found': list(topics),
                    'source_files': list(sources),
                    'sample_content': sample_docs[0].page_content[:200] if sample_docs else None
                }
            else:
                return {
                    'total_documents': 0,
                    'topics_found': [],
                    'source_files': [],
                    'sample_content': None
                }
                
        except Exception as e:
            return {'error': f'Stats lek√©r√©si hiba: {e}'}
    
    def test_retrieval(self, query: str, k: int = 3) -> Dict[str, Any]:
        """‚úÖ JAV√çTVA: Retrieval tesztel√©se"""
        try:
            if not self.vectorstore:
                return {'error': 'Vector store nincs inicializ√°lva'}
            
            # Similarity search
            docs = self.vectorstore.similarity_search(query, k=k)
            
            results = []
            for i, doc in enumerate(docs):
                results.append({
                    'rank': i + 1,
                    'content': doc.page_content[:300],
                    'metadata': doc.metadata,
                    'content_length': len(doc.page_content)
                })
            
            return {
                'query': query,
                'results_count': len(results),
                'results': results
            }
            
        except Exception as e:
            return {'error': f'Retrieval teszt hiba: {e}'}

# =============================================================================
# HI√ÅNYZ√ì F√úGGV√âNY HOZZ√ÅAD√ÅSA - kompatibilit√°shoz
# =============================================================================

def run_rag_analysis(patient_data: Dict[str, Any], openai_api_key: str = None) -> Dict[str, Any]:
    """
    ‚úÖ JAV√çTVA: RAG elemz√©s futtat√°sa - kompatibilit√°s f√ºggv√©ny
    
    Args:
        patient_data: Beteg adatok (a session state-b≈ël)
        openai_api_key: OpenAI API kulcs (opcion√°lis)
        
    Returns:
        Dict: RAG elemz√©s eredm√©nye
    """
    try:
        st.info("üîç RAG alap√∫ elemz√©s ind√≠t√°sa...")
        
        # API kulcs ellen≈ërz√©se
        if not openai_api_key:
            openai_api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
        
        if not openai_api_key:
            st.error("‚ùå OpenAI API kulcs nem tal√°lhat√≥!")
            return _create_empty_result()
        
        # RAG Analyzer inicializ√°l√°sa
        analyzer = RAGAnalyzer()
        
        # Elemz√©s futtat√°sa
        st.info("ü§ñ AI elemz√©s futtat√°sa...")
        results = analyzer.analyze_medical_case(patient_data)
        
        # Eredm√©nyek ellen≈ërz√©se
        if not results.get('success', False):
            st.warning(f"‚ö†Ô∏è RAG elemz√©s probl√©m√°ba √ºtk√∂z√∂tt: {results.get('error', 'Ismeretlen hiba')}")
            return _create_empty_result()
        
        # Eredm√©nyek ment√©se
        save_paths = _save_rag_results(results, patient_data)
        
        # UI megjelen√≠t√©s
        _display_rag_results(results, save_paths)
        
        st.success("‚úÖ RAG elemz√©s sikeresen befejezve!")
        return results
        
    except Exception as e:
        st.error(f"‚ùå RAG elemz√©si hiba: {e}")
        print(f"RAG hiba r√©szletei: {e}")
        return _create_empty_result()

def _create_empty_result() -> Dict[str, Any]:
    """√úres eredm√©ny strukt√∫ra"""
    return {
        'success': False,
        'patient_condition': "Nem √°ll rendelkez√©sre inform√°ci√≥",
        'symptom_management': "Nem √°ll rendelkez√©sre inform√°ci√≥", 
        'recommended_specialist': "Nem √°ll rendelkez√©sre inform√°ci√≥",
        'additional_info': "Nem √°ll rendelkez√©sre inform√°ci√≥",
        'timestamp': datetime.now().isoformat(),
        'sources': []
    }

def _save_rag_results(results: Dict[str, Any], patient_data: Dict[str, Any]) -> Dict[str, str]:
    """RAG eredm√©nyek ment√©se"""
    try:
        export_path = Path("rag_data/exports")
        export_path.mkdir(parents=True, exist_ok=True)
        
        # Export adat √∂ssze√°ll√≠t√°sa
        case_id = patient_data.get('case_id', f"rag_{datetime.now().strftime('%Y%m%d%H%M%S')}")
        export_data = {
            "rag_analysis": results,
            "patient_data": patient_data,
            "analysis_timestamp": results.get('timestamp', datetime.now().isoformat()),
            "case_id": case_id
        }
        
        # JSON ment√©s
        json_path = export_path / f"{case_id}_rag.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        return {
            "json_path": str(json_path),
            "pdf_path": None  # PDF gener√°l√°s opcion√°lis
        }
        
    except Exception as e:
        print(f"RAG eredm√©ny ment√©si hiba: {e}")
        return {"json_path": None, "pdf_path": None}

def _display_rag_results(results: Dict[str, Any], save_paths: Dict[str, str]):
    """RAG eredm√©nyek megjelen√≠t√©se"""
    st.markdown("### üß† RAG Elemz√©s Eredm√©nye")
    
    # Eredm√©nyek megjelen√≠t√©se expander-ekben
    with st.expander("üìã Beteg √°llapota", expanded=True):
        st.markdown(results.get('patient_condition', 'Nincs inform√°ci√≥'))
    
    with st.expander("üíä Mit tehet a t√ºnetek ellen", expanded=True):
        st.markdown(results.get('symptom_management', 'Nincs inform√°ci√≥'))
    
    with st.expander("üë®‚Äç‚öïÔ∏è Aj√°nlott szakorvos", expanded=True):
        st.markdown(results.get('recommended_specialist', 'Nincs inform√°ci√≥'))
    
    with st.expander("‚ÑπÔ∏è Tov√°bbi inform√°ci√≥k", expanded=True):
        st.markdown(results.get('additional_info', 'Nincs inform√°ci√≥'))
    
    # Forr√°sok megjelen√≠t√©se
    if results.get('sources'):
        st.info(f"üìö Felhaszn√°lt forr√°sok: {', '.join(results['sources'])}")
    
    # Let√∂lt√©si lehet≈ës√©gek
    if save_paths.get('json_path'):
        st.markdown("### üì• Let√∂lt√©s")
        try:
            with open(save_paths['json_path'], 'r', encoding='utf-8') as f:
                st.download_button(
                    label="üìÑ RAG JSON let√∂lt√©se",
                    data=f.read(),
                    file_name=Path(save_paths['json_path']).name,
                    mime="application/json"
                )
        except Exception as e:
            st.error(f"Let√∂lt√©si hiba: {e}")